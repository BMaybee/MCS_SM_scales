from sm_scales_utils import *
import cartopy.geodesic as cgeo
from scipy.ndimage.measurements import label

### ! WARNING: THIS SCRIPT IS DESIGNED TO RUN ON MACHINES WHICH PERMIT SERIAL PARALLEL PROCESSING ! ###

#### Main function for extracting spatial composite of field about point:
# - field : dataArray for variable, with coordinates in form ([vertical_coord],latitude,longitude), where lat and lon are both ascending, and vertical_coordinate is optional
# - clat, clon : the sample point coordinates
# - size : pixel size of x and y axis of composite. For 1.5km grid -> 1.5x<size> km box
# Returns numpy array.
def composite_field(field,clat,clon,size=801): # 800 corresponds to 1200km box
    s=len(field.shape)
    if s==2:
        field=field.expand_dims("pressure",axis=0)
    # convert pixel length to approximate degrees equivalent. Assuming Cartesian, and parameter derived empirically! Should be improved.
    ll_border=int(size/133) # converts 801 to 6 degrees; 401 to 3 degrees
    field_comp=field.sel(latitude=slice(clat-ll_border,clat+ll_border),
                         longitude=slice(clon-ll_border,clon+ll_border))
    # field_comp in current form can vary a bit in size. We need everything exactly the same. Thus zoom in on central size x size locus.
    l1,l2=field_comp.shape[1],field_comp.shape[2]
    field_comp=field_comp[:,int(l1/2 - size/2):int(l1/2 + size/2 + 2),int(l2/2 - size/2):int(l2/2 + size/2 +2)]
    field_comp=field_comp[:,:size,:size]
    if field_comp.shape[1] != size or field_comp.shape[2] != size: # This error is typically generated by sampling a point too close to edge of data domain for specified size of composite
        print("Composite sampling error, ",field_comp.shape,clat,clon)
    """
    #don't need to worry about edges, have nice big model domain and only taking composites from the centre
    pad=999999
    if field_comp.shape[-2] == size:
        pass
    else:
        field_comp=field_comp+pad
        if clat>maxlat-ll_border:
            field_comp=field_comp.pad(latitude=(0,size-field_comp.shape[-2]),constant_values=0)
        else:
            field_comp=field_comp.pad(latitude=(size-field_comp.shape[-2],0),constant_values=0)
        field_comp=field_comp.where(field_comp>0)-pad

    if field_comp.shape[-1] == size:
        pass
    else:
        field_comp=field_comp+pad
        if clon>maxlon-ll_border:
            field_comp=field_comp.pad(longitude=(0,size-field_comp.shape[-1]),constant_values=0)
        else:
            field_comp=field_comp.pad(longitude=(size-field_comp.shape[-1],0),constant_values=0)
        field_comp=field_comp.where(field_comp>0)-pad
    """
    if s==2:
        field_comp=field_comp.isel(pressure=0)
    return field_comp.values
    

psize=10 # Sets up Pool size - this is the number of parallel cores needed. Must be specified if submitting to batch processor.

###################################################################################
# - sim : ["control","sens"] - for continuous 40 day Control, or ANY set of 48hr simulations, respectively. All paper results use Sens.
# - expt : if sens, experiment to select. Options: ["control_48hr_runs", "wg_mcs", "large_only"]
# - day : if sens, day of experiment to sample (typically wish to split). Options 0 or 1, i.e. pythonic indexing.
# - method : specification of the type of sampling point being composited. Options ["core","diffPmax","diffPmaxWET"] (last two being "MesoDRY" and "MesoWET" SM patches, first MCS cores)
# - loc_time : the hour at which sampling points have been identified, UTC. Default 17 for "core", 9 for SM patches
# - samp_time : the hour at which wish to sample the ambient fields, UTC. Default 12
# - fields : the group of fields for which to calculate composites
#     - "2dfields" : 2D composite of SM anom, q925 anom, T925 anom, T2m, q2m, 2m Td, u650, u925, v925, u650-u925 shear, 925hPa horizontal divergence, TCW anom, TCC anom, rainfall, PBL depth, TOA brightness T
#     - "hfx_accums" : anomalous and absolute accumulated surface fluxes over 4 hours up to (and including) samp_time. sensible hfx, latent lhfx, available energy, evap frac, net SW, net LW.
#                      Also SM 3 at samp_time - 3 ("pre_sm")
#     - "plevs": anomalous and absolute pressure-level composites up to 500hPa o T, q, Z, u, v. Can include horizontal_divergence (MUCH SLOWER - UNCOMMENT VAR)
#     - "3dfields" : model-level w, \theta, q, p. Anomalies for q and \theta. Var names specify vertical coordinate grid.
#     - "precip_accums" : 12 hour rainfall accumulations and probability of accum > 5mm/hr before (PRE) and after (POST) samp_time. Also SM anomaly 12 hours before and 12 hours after.
#     - "MORNprecip_accums" : rainfall accum between 9 and 12 UTC
#     - "tracers" : 40-day Control ONLY. RainEvap, Boundary Layer and Boundary Layer SM moisture tracer fields
#     - "synoptic" : anomalies of TCC and 700hPa u, v, Z and PV (designed to think about AEWs)
#     - "MCSstructure" : full-depth pressure-level +/- 300km composite anomalies of T, q, Z, u, v, PV, pressure tendency (\omega), and horizontal divergence. 
#     - "instability" : pressure-level +/- 300km composites up to 500hPa of equivalent potential temperature, its anomaly, CAPE proxy, and temperature
# - fieldsim : functionality to swap around the day of the composite fields vs sample points; not recommended....
# - filtstr : only relevant if method="core". Options ["core","mcs"]. Use "mcs" for all identified cores, "core" for filtered sample used for LA interactions.
# - split : integer. Some (3D) field composites get very memory intense. This parameter enables further parallelisation by splitting the workflow into 4 complementary runs. Designed to be used with slurm sbatch_array
###################################################################################

parser = argparse.ArgumentParser()
parser.add_argument("-s", "--sim", required=True) 
parser.add_argument("-e", "--expt", required=False, default="control_48hr_runs")
parser.add_argument("-d", "--day", required=False, type=int, default=0)
parser.add_argument("-m", "--method", required=False, default="core")
parser.add_argument("-lt", "--loc_time", required=False, default=17, type=int)
parser.add_argument("-st", "--samp_time", required=False, default=12, type=int)
parser.add_argument("-f", "--fields", required=False, default="soil_moisture")
parser.add_argument("-fs", "--fieldsim", required=False) 
parser.add_argument("-flt", "--filtstr", required=False, default="filt")
parser.add_argument("-split","--split",type=int,required=False)
args = parser.parse_args()

sim=args.sim.lower()
expt=args.expt
sim_day=args.day
if "control" in sim:
    sim2="Control_run"
else:
    period=period[:-2]
    sim2=expt.capitalize()+"_D%s"%(sim_day+1)
            
method=args.method
loc_hr=args.loc_time
samp_hr=args.samp_time
fields=args.fields
filtstr=args.filtstr

if args.split is not None:
    period=period[args.split*12:(args.split+1)*12]
    psize=3
    if len(period)<12:
        psize=1

#### Get sample points ####
# SM patches
if "diffPmax" in method:
    samp_locs=pd.read_csv("~/LMCS/LMCS_Wafrica_sim/field_scales/diffPmax_tables/Wg_mcs_D1_sm-diff_{:02d}Z_pmax_locs.csv".format(loc_hr))
    csize=801
    if method=="diffPmax":
        samp_locs=samp_locs[samp_locs["diff_sign"]==1] # want to sample around DRY patches in control
    elif method=="diffPmaxWET":
        samp_locs=samp_locs[samp_locs["diff_sign"]==-1]
    loc_x,loc_y="pmax_lon","pmax_lat"

# MCS cores
else:
    if filtstr=="mcs": # more interested in MCS-time composites, not precursor LA interactions. So use full core population
        samp_locs=pd.read_csv("~/LMCS/LMCS_Wafrica_sim/MCS_analysis/Tables/{}_MCS_{:02d}Z_Sahel_NOfilt_cores.csv".format(sim2,loc_hr))
        csize=401 # axis length of composites - smaller for MCS-relative to focus on updraft scales
    else: # Default - use filtered set.
        samp_locs=pd.read_csv("~/LMCS/LMCS_Wafrica_sim/MCS_analysis/Tables/{}_MCS_{:02d}Z_Sahel_filt_cores.csv".format(sim2,loc_hr))
        csize=801
    loc_x,loc_y="core_lon","core_lat"

    if sim=="sens": # deal with just one of D1 and D2 cores, as interested in difference.
        samp_locs=samp_locs[samp_locs.sim_day==sim_day]

if expt!="control_48hr_runs": # required to find file paths
        expt="sensitivity_runs_"+expt
        
#RESTRICT COMPOSITES TO SAHEL
samp_locs=samp_locs[(samp_locs[loc_y]>9) & (samp_locs[loc_y]<=19) & (samp_locs[loc_x]>=348) & (samp_locs[loc_x]<=378)]
if args.fieldsim is not None:
    try:
        sim_day=int(args.fieldsim)
        fieldsim="D%s-"%(sim_day+1)
    except:
        sim=args.fieldsim
        fieldsim=sim2.replace("_","")+"-"

orog=xr.DataArray.from_iris(iris.load("/gws/nopw/j04/lmcs/hburns/Test_run/ancils/qrparm.orog")[9])
sm_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_run_mean_sm_1p5km.nc").STASH_m01s08i223[:,:2200,:3300]


if fields=="2dfields":
    field_vars=["sm_anom","q925_anom","t925_anom","t2","q2","td2","u650","u925","v925","ushear650_925","horizontal_divergence",
                "tcw_anom","tcc_anom","precip","pblh","bt"]
elif fields=="hfx_accums":
    field_vars=["pre_sm","shfx","lhfx","ae","ef","sw_nsfc","lw_nsfc"]
elif fields=="plevs":
    field_vars=["t_plevs","z_plevs","u_plevs","v_plevs","q_plevs"]#,"horizontal_divergence_plevs"
    # n.b. including divergence is desirable, but significantly slower.
    lid=11 # for 3D composites, we don't want upper levels as primary concern is PBL development. Cuts off levels at ~500hPa
elif fields=="3dfields":
# NOTE: DO NOT use theta or rho in a physical variable name - they are reserved strings for handling the UM native grid coordinates.
    field_vars=["w_theta","pt_theta","q_theta","p_theta"]#"u_rho","v_rho",
    lid=32
elif fields=="precip_accums":
    field_vars=["precip_accum_pre","precip_accum_post","soil_moisture_post","soil_moisture_pre"]
elif fields=="MORNprecip_accums":
    samp_hr=12
    field_vars=["precip_accum_pre"]
elif fields=="tracers":
    field_vars=["rainEvap","boundaryLayer","boundaryLayerSM"]
elif fields=="synoptic":
    field_vars=["u700","v700","z700","pv700","tcc"]
elif fields=="MCSstructure":
    field_vars=["t_plevs_anom","q_plevs_anom","z_plevs_anom","u_plevs","v_plevs","pv_plevs_anom","omega_plevs","horizontal_divergence_plevs",
               "theta_e_plevs_anom"]
    lid=21 # for MCS 3D fields include full troposphere
    csize=401
elif fields=="instability":
    field_vars=["theta_e_plevs","capeprox_plevs","t_plevs"]
    lid=11
    csize=401
    
try:
    print(field_vars)
except:
    raise("Error: invalid field set entered")

###### ALL ROUTINES NESTED WITHIN THIS FUNCTION ######
# Parallelisation done on dates within period. Split period into 10 groups of 4 days. Idx specifies the group.

def parallelise(idx):
    # master dictionary in which composites stored for different variables. Converted to dataset at end.
    # Stores numpy arrays of zeros for variable, count and variable anomalies (if included). Vastly reduces computation times of long sets of means.
    comp={}
    if fields in ["3dfields","plevs"]:
        for field in field_vars:
            comp[field]=np.zeros((lid,801,801))
            comp[field+"_count"]=np.zeros((lid,801,801))
            if fields=="plevs":
                if field!="horizontal_divergence_plevs":
                    comp[field+"_anom"]=np.zeros((lid,801,801))
            elif fields=="3dfields":
                comp["pt_theta_anom"]=np.zeros((lid,801,801))
                comp["q_theta_anom"]=np.zeros((lid,801,801))
    elif fields in ["MCSstructure","instability"]:
        for field in field_vars:
            comp[field]=np.zeros((lid,401,401))
            comp[field+"_count"]=np.zeros((lid,401,401))
            if fields=="instability":
                comp["theta_e_plevs_anom"]=np.zeros((lid,401,401))
    else:
        for field in field_vars:
            comp[field]=np.zeros((801,801))
            comp[field+"_count"]=np.zeros((801,801))
            if fields in ["hfx_accums","synoptic","precip_accums"]:
                comp[field+"_anom"]=np.zeros((801,801))
            if fields=="2dfields":
                comp["ushear650_925_anom"]=np.zeros((801,801))
                comp["pblh_anom"]=np.zeros((801,801))

    if fields=="MORNprecip_accums":
        comp["precip_accum_pre_prob"]=np.zeros((801,801))
    elif fields=="precip_accums":
        comp["precip_accum_pre_prob"]=np.zeros((801,801))
        comp["precip_accum_post_prob"]=np.zeros((801,801))

    # load climatology fields to take anomalies from (all Control hourly means)
    if fields=="2dfields":
        sm_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_sm_1p5km.nc"%(sim_day+1)
                               ).STASH_m01s08i223.sel(hour=samp_hr%24)[:2200,:3300]
        tcw_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_tcw_1p5km.nc"%(sim_day+1)
                                ).STASH_m01s30i461.sel(hour=samp_hr%24)[:2200,:3300]
        tcc_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_tcc_1p5km.nc"%(sim_day+1)
                                ).STASH_m01s09i217.sel(hour=samp_hr%24)[:2200,:3300]
        pbl_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_pbl_depth_1p5km.nc"%(sim_day+1)
                                )[stash_dict["pbl_depth"]].sel(hour=samp_hr%24)[:2200,:3300]
        q925_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_q925_1p5km.nc"%(sim_day+1)
                                 ).STASH_m01s30i205.sel(hour=samp_hr%24)[:2200,:3300]
        t925_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_t925_1p5km.nc"%(sim_day+1)
                                 ).STASH_m01s30i204.sel(hour=samp_hr%24)[:2200,:3300]
        shear_clim=(xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_u650_1p5km.nc"%(sim_day+1)
                                   ).STASH_m01s30i201.sel(hour=samp_hr%24)[:2200,:3300] 
                    - xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_u925_1p5km.nc"%(sim_day+1)
                                     ).STASH_m01s30i201.sel(hour=samp_hr%24)[:2200,:3300])
        #v925_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_run_mean_v925_1p5km.nc").STASH_m01s30i202.sel(hour=samp_hr)[:2200,:3300]
        #t2_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_run_mean_%02dZ_t2_1p5km.nc"%samp_hr).STASH_m01s03i236.isel(hour=0)[:2200,:3300]
        #q2_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_run_mean_%02dZ_q2_1p5km.nc"%samp_hr).STASH_m01s03i237.sel(hour=samp_hr-1)[:2200,:3300]

    elif fields=="hfx_accums":
        sm_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_sm_1p5km.nc"%(sim_day+1)
                               ).STASH_m01s08i223.sel(hour=(samp_hr-3)%24)[:2200,:3300]
        # for fluxes do not reduce to single hour - need multiple for accum anomalies
        lh_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_lhfx_1p5km.nc"%(sim_day+1)
                               ).STASH_m01s03i234[:,:2200,:3300]
        sh_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_shfx_1p5km.nc"%(sim_day+1)
                               ).STASH_m01s03i217[:,:2200,:3300]
        sw_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_sw_nsfc_1p5km.nc"%(sim_day+1)
                               ).STASH_m01s01i201
        lw_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_lw_nsfc_1p5km.nc"%(sim_day+1)
                               ).STASH_m01s02i201

    elif fields=="plevs":
        q_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_q_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s30i205[1:lid+1,:,:]
        # Mean states for 3D vars stored at coarser ~10km res to save space. Thus must reinterpolate back to native grid.
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["q_plevs"])[0,:,:2200,:3300]
        #w_clim=w_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        q_clim=q_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        t_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_t_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s30i204[1:lid+1,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["t_plevs"])[0,:,:2200,:3300]
        t_clim=t_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        z_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_z_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s16i202[1:lid+1,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["z_plevs"])[0,:,:2200,:3300]
        z_clim=z_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        u_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_u_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s30i201[1:lid+1,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["u_plevs"])[0,:,:2200,:3300]
        #w_clim=w_clim.interp(latitude=ref.latitude,longitude=ref.longitude
        u_clim=u_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        v_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_v_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s30i202[1:lid+1,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["v_plevs"])[0,:,:2200,:3300]
        v_clim=v_clim.interp(latitude=ref.latitude,longitude=ref.longitude)

    elif fields=="precip_accums":
        sm_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_sm_1p5km.nc"%(sim_day+1)).STASH_m01s08i223[:,:2200,:3300]
        precip_clim=3600*xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_precip_1p5km.nc"%(sim_day+1)).STASH_m01s04i203[:,:2200,:3300]
    
    elif fields=="3dfields":
        pt_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_pt_theta_levs_10km.nc"%(sim_day+1,samp_hr%24)
                               ).STASH_m01s00i004.isel(hour=0)[:lid,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"3D_vars_hourly",stash_dict["pt_theta_levs"])[0,:,:2200,:3300]
        #w_clim=w_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        pt_clim=pt_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        q_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_q_theta_levs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).STASH_m01s00i010.isel(hour=0)[:lid,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"3D_vars_hourly",stash_dict["q_theta_levs"])[0,:,:2200,:3300]
        q_clim=q_clim.interp(latitude=ref.latitude,longitude=ref.longitude)

        
    elif fields=="synoptic":
        u700_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_u700_1p5km.nc"%(sim_day+1,samp_hr)
                                 ).STASH_m01s30i201.sel(hour=samp_hr)[:2200,:3300]
        v700_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_v700_1p5km.nc"%(sim_day+1,samp_hr)
                                ).STASH_m01s30i202.sel(hour=samp_hr)[:2200,:3300]
        z700_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_z700_1p5km.nc"%(sim_day+1,samp_hr)
                                 ).STASH_m01s16i202.sel(hour=samp_hr)[:2200,:3300]
        pv700_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_pv700_1p5km.nc"%(sim_day+1,samp_hr)
                                  ).STASH_m01s15i229.sel(hour=samp_hr)[:2200,:3300]
        tcc_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_tcc_1p5km.nc"%(sim_day+1)
                                ).STASH_m01s09i217.sel(hour=samp_hr)[:2200,:3300]

    elif fields=="MCSstructure":
        q_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_q_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s30i205[1:,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["q_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
        q_clim=q_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        
        t_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_t_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s30i204[1:,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["t_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
        t_clim=t_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        
        z_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_z_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s16i202[1:,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["z_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
        z_clim=z_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        
        u_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_u_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s30i201[1:,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["u_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
        u_clim=u_clim.interp(latitude=ref.latitude,longitude=ref.longitude)
        
        v_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_v_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                              ).sel(hour=samp_hr%24).STASH_m01s30i202[1:,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["v_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
        v_clim=v_clim.interp(latitude=ref.latitude,longitude=ref.longitude)

        te_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_theta_e_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                               ).sel(hour=samp_hr%24).equivalent_potential_temperature[1:,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["t_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
        te_clim=te_clim.interp(latitude=ref.latitude,longitude=ref.longitude)

    
    elif fields=="instability":
        te_clim=xr.open_dataset("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/mean_states/Control_48hr_runs_D%s_mean_%02dZ_theta_e_plevs_10km.nc"%(sim_day+1,samp_hr%24)
                               ).sel(hour=samp_hr%24).equivalent_potential_temperature[1:lid+1,:,:]
        ref=load_file(sim,pd.Timestamp("2006-07-27"),12,"model-diagnostics",stash_dict["t_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,lid))
        te_clim=te_clim.interp(latitude=ref.latitude,longitude=ref.longitude)

    
    ################# DAILY FILE SELECTION ######################
    # Segmentation of full period for serial paraellelisation
    period_segment=period[int(4*idx):int(4*(idx+1))]

    # First load up full domain files for each day
    for j, date in enumerate(period_segment):
        print(date)

        # Have to be careful about selection of right MCSs - to access Day 2 in sensitivity experiment, need Day 1 date + hour > 24; thus need to shift table selection date forwards
        # However we don't want to do that if trying to use fields from one day, and storms from the other!
        if args.fieldsim is None:
            date2=date+pd.Timedelta(sim_day,"d")
        elif sim=="sens":
            date2=date+pd.Timedelta((sim_day+1)%2,"d")
            
        day_data=samp_locs[(samp_locs["day"]==date2.day) & (samp_locs["month"]==date2.month) & (samp_locs["hour"]==loc_hr)]
        data_dict={}
        
        if fields=="2dfields":               
            data_dict["sm_anom"]=load_file(sim,date,samp_hr+sim_day*24,"soil_moistures",stash_dict["sm"])[0,:,:] - sm_clim
            # include conversion to mm/hr
            data_dict["precip"]=3600*load_file(sim,date,samp_hr+sim_day*24,"surface_vars",stash_dict["precip"])[0,:,:]
            data_dict["tcw_anom"]=load_file(sim,date,samp_hr+sim_day*24,"surface_vars",stash_dict["tcw"])[0,:,:] - tcw_clim
            data_dict["tcc_anom"]=load_file(sim,date,samp_hr+sim_day*24,"surface_vars",stash_dict["tcc"])[0,:,:] - tcc_clim
            data_dict["t2"]=load_file(sim,date,samp_hr+sim_day*24,"surface_vars",stash_dict["t2"])[0,:,:]
            data_dict["q2"]=load_file(sim,date,samp_hr+sim_day*24,"surface_vars",stash_dict["q2"])[0,:,:]
            data_dict["td2"]=load_file(sim,date,samp_hr+sim_day*24,"surface_vars",stash_dict["td2"])[0,:,:]
            data_dict["pblh"]=load_file(sim,date,samp_hr+sim_day*24,"surface_vars",stash_dict["pbl_depth"])[0,:,:]
            data_dict["pblh_anom"]=data_dict["pblh"] - pbl_clim
            bt=load_file(sim,date,samp_hr+sim_day*24,"surface_vars",stash_dict["toaolr"])[0,:,:]
            data_dict["bt"]=tb_from_olr(bt)
            
            u925=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["u_plevs"]).sel(PLEVS=925)[0,:,:]
            v925=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["v_plevs"]).sel(PLEVS=925)[0,:,:]
            data_dict["u650"]=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["u_plevs"]).sel(PLEVS=650)[0,:,:]
            data_dict["ushear650_925"]=data_dict["u650"] - u925.where(u925!=0)
            data_dict["ushear650_925_anom"]=data_dict["ushear650_925"] - shear_clim
            t925=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["t_plevs"]).sel(PLEVS=925)[0,:,:]
            q925=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["q_plevs"]).sel(PLEVS=925)[0,:,:]

            # mask out sub-surface plevel values, which are stored as zeros.
            data_dict["u925"]=u925.where(u925!=0)
            data_dict["v925"]=v925.where(v925!=0)
            data_dict["t925_anom"]=t925.where(t925!=0) - t925_clim
            data_dict["q925_anom"]=q925.where(q925!=0) - q925_clim
            div=mpcalc.divergence(data_dict["u925"],data_dict["v925"],crs=ccrs.PlateCarree())         
            data_dict["horizontal_divergence"]=div.metpy.dequantify()

        
        elif fields=="hfx_accums":
            data_dict["pre_sm"] = load_file(sim,date,samp_hr-3+sim_day*24,"soil_moistures",stash_dict["sm"])[0,:,:]
            data_dict["pre_sm_anom"] = data_dict["pre_sm"]-sm_clim
            lh_accum=load_file(sim,date,samp_hr-3+sim_day*24,"surface_vars",stash_dict["lhfx"])[0,:,:]
            if sim=="control": # problems with sensible heat flux
                sh_accum=load_file(sim,date,samp_hr-3+sim_day*24,"sensible_hfx_control",stash_dict["shfx"])[0,:,:].interp(latitude=lh_accum.latitude)
            else:
                sh_accum=load_file(sim,date,samp_hr-3+sim_day*24,"surface_vars",stash_dict["shfx"])[0,:,:].interp(latitude=lh_accum.latitude)
            sw_accum=load_file(sim,date,samp_hr-3+sim_day*24,"surface_vars",stash_dict["sw_nsfc"])[0,:,:]
            lw_accum=load_file(sim,date,samp_hr-3+sim_day*24,"surface_vars",stash_dict["lw_nsfc"])[0,:,:]
            
            for hr in np.arange(samp_hr-2,samp_hr+1):
                if sim=="control":
                    sh=load_file(sim,date,hr,"sensible_hfx_control",stash_dict["shfx"])[0,:,:].interp(latitude=lh_accum.latitude)
                else:
                    sh=load_file(sim,date,hr+sim_day*24,"surface_vars",stash_dict["shfx"])[0,:,:].interp(latitude=lh_accum.latitude)
                lh=load_file(sim,date,hr+sim_day*24,"surface_vars",stash_dict["lhfx"])[0,:,:] 
                sh_accum=sh + sh_accum
                lh_accum=lh + lh_accum
                sw=load_file(sim,date,hr+sim_day*24,"surface_vars",stash_dict["sw_nsfc"])[0,:,:]
                lw=load_file(sim,date,hr+sim_day*24,"surface_vars",stash_dict["lw_nsfc"])[0,:,:]
                sw_accum=sw + sw_accum
                lw_accum=lw + lw_accum
            data_dict["shfx"]=sh_accum
            data_dict["lhfx"]=lh_accum
            data_dict["ae"]=lh_accum+sh_accum
            data_dict["ef"]=lh_accum/data_dict["ae"].where(data_dict["ae"]!=0)
            data_dict["sw_nsfc"]=sw_accum
            data_dict["lw_nsfc"]=lw_accum
            
            data_dict["shfx_anom"] = sh_accum - sh_clim.sel(hour=slice(samp_hr-3,samp_hr)).sum(dim="hour")
            data_dict["lhfx_anom"] = lh_accum - lh_clim.sel(hour=slice(samp_hr-3,samp_hr)).sum(dim="hour")
            data_dict["ae_anom"] = data_dict["shfx_anom"] + data_dict["lhfx_anom"]
            ef_clim = (lh_clim/(sh_clim+lh_clim)).sel(hour=slice(samp_hr-3,samp_hr)).sum(dim="hour")
            data_dict["ef_anom"] = data_dict["ef"] - ef_clim.where(ef_clim!=0)
            data_dict["sw_nsfc_anom"] = sw_accum - sw_clim.sel(hour=slice(samp_hr-3,samp_hr)).sum(dim="hour")
            data_dict["lw_nsfc_anom"] = lw_accum - lw_clim.sel(hour=slice(samp_hr-3,samp_hr)).sum(dim="hour")
        
        
        elif "precip_accums" in fields:
            # all precip fields converted from kg/m2/s to mm/hr upon loading
            if "MORN" in fields:
                precip_accum_pre=3600*load_file(sim,date,9+sim_day*24,"surface_vars",stash_dict["precip"])[0,:,:]
                for hr in np.arange(10,13):
                    precip=3600*load_file(sim,date,hr+sim_day*24,"surface_vars",stash_dict["precip"])[0,:,:]
                    precip_accum_pre=precip + precip_accum_pre
                data_dict["precip_accum_pre"]=precip_accum_pre
            else:
                precip_accum_pre=3600*load_file(sim,date,7+sim_day*24,"surface_vars",stash_dict["precip"])[0,:,:]
                for hr in np.arange(8,19):
                    precip=3600*load_file(sim,date,hr+sim_day*24,"surface_vars",stash_dict["precip"])[0,:,:]
                    precip_accum_pre=precip + precip_accum_pre
    
                precip_accum_post=3600*load_file(sim,date,18+sim_day*24,"surface_vars",stash_dict["precip"])[0,:,:]
                for hr in np.arange(19,31):
                    precip=3600*load_file(sim,date,hr+sim_day*24,"surface_vars",stash_dict["precip"])[0,:,:]
                    precip_accum_post=precip + precip_accum_post
                    
                data_dict["precip_accum_pre"]=precip_accum_pre
                data_dict["precip_accum_pre_anom"]=precip_accum_pre - precip_clim.sel(hour=slice(7,18)).sum(dim="hour")
                data_dict["precip_accum_post"]=precip_accum_post
                data_dict["precip_accum_post_anom"]=precip_accum_post - precip_clim.sel(hour=slice(19,24)).sum(dim="hour") - precip_clim.sel(hour=slice(0,6)).sum(dim="hour")
                data_dict["soil_moisture_pre"]=load_file(sim,date,7+sim_day*24,"soil_moistures",stash_dict["sm"])[0,:,:]
                data_dict["soil_moisture_pre_anom"] = data_dict["soil_moisture_pre"] - sm_clim.sel(hour=7)
                data_dict["soil_moisture_post"]=load_file(sim,date,30+sim_day*24,"soil_moistures",stash_dict["sm"])[0,:,:]
                data_dict["soil_moisture_post_anom"] = data_dict["soil_moisture_post"] - sm_clim.sel(hour=6)

        
        elif fields=="plevs":
            u=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["u_plevs"])[0,1:lid+1,:2200,:3300].rename({"PLEVS":"pressure"})  # with lid=11, corresponds to 950 - 500 hPa inclusive
            v=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["v_plevs"])[0,1:lid+1,:2200,:3300].rename({"PLEVS":"pressure"})
            
            data_dict["u_plevs"]=u.where(u!=0)
            data_dict["v_plevs"]=v.where(v!=0)
            q=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["q_plevs"])[0,1:lid+1,:2200,:3300].rename({"PLEVS":"pressure"})
            t=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["t_plevs"])[0,1:lid+1,:2200,:3300].rename({"PLEVS":"pressure"})
            z=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["z_plevs"])[0,1:lid+1,:2200,:3300].rename({"PLEVS":"pressure"})

            data_dict["q_plevs"]=q.where(q!=0)
            data_dict["t_plevs"]=t.where(t!=0)
            data_dict["z_plevs"]=z.where(z!=0)

            data_dict["u_plevs_anom"]=data_dict["u_plevs"] - u_clim
            data_dict["v_plevs_anom"]=data_dict["v_plevs"] - v_clim
            data_dict["q_plevs_anom"]=data_dict["q_plevs"] - q_clim
            data_dict["t_plevs_anom"]=data_dict["t_plevs"] - t_clim
            data_dict["z_plevs_anom"]=data_dict["z_plevs"] - z_clim
            
            if "horizontal_divergence_plevs" in field_vars:
                div=mpcalc.divergence(data_dict["u_plevs"],data_dict["v_plevs"],crs=ccrs.PlateCarree())         
                data_dict["horizontal_divergence_plevs"]=div.metpy.dequantify()

        
        elif fields=="3dfields":            
            data_dict["w_theta"]=load_file(sim,date,samp_hr+sim_day*24,"3D_vars_hourly",stash_dict["w_theta_levs"])[0,:lid,:2200,:3300] # limit to BL fields to save memory
            data_dict["pt_theta"]=load_file(sim,date,samp_hr+sim_day*24,"3D_vars_hourly",stash_dict["pt_theta_levs"])[0,:lid,:2200,:3300]
            data_dict["pt_theta_anom"] = data_dict["pt_theta"] - pt_clim

            #data_dict["u_rho"]=load_file(sim,date,samp_hr+sim_day*24,"3D_vars_hourly",stash_dict["u_rho_levs"])[0,:lid,:2200,:3300] # selection range restricts to heights < 4000m ~ 650hPa.
            #v=load_file(sim,date,samp_hr+sim_day*24,"3D_vars_hourly",stash_dict["v_rho_levs"])[0,:lid,:2200,:3300]
            #data_dict["v_rho"]=v.interp(latitude=data_dict["u_rho"].latitude,longitude=data_dict["u_rho"].longitude)
            
            data_dict["q_theta"]=load_file(sim,date,samp_hr+sim_day*24,"3D_vars_hourly",stash_dict["q_theta_levs"])[0,:lid,:2200,:3300]
            data_dict["q_theta_anom"] = data_dict["q_theta"] - q_clim
            data_dict["p_theta"] = load_file(sim,date,samp_hr+sim_day*24,"3D_vars_hourly",stash_dict["p_theta_levs"])[0,:lid,:2200,:3300]
        
            #div=mpcalc.divergence(data_dict["u_rho"],data_dict["v_rho"],crs=ccrs.PlateCarree())         
            #data_dict["horizontal_divergence"]=div.metpy.dequantify()

        
        elif fields=="synoptic":
            data_dict["u700"]=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["u_plevs"]).sel(PLEVS=700)[0,:2200,:3300]
            data_dict["v700"]=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["v_plevs"]).sel(PLEVS=700)[0,:2200,:3300]
            data_dict["z700"]=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["z_plevs"]).sel(PLEVS=700)[0,:2200,:3300]
            data_dict["pv700"]=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["pv_plevs"]).sel(PLEVS=700)[0,:2200,:3300]            
            data_dict["tcc"]=load_file(sim,date,samp_hr+sim_day*24,"surface_vars",stash_dict["tcc"])[0,:,:]

            data_dict["u700_anom"]=data_dict["u700"]-u700_clim
            data_dict["v700_anom"]=data_dict["v700"]-v700_clim
            data_dict["z700_anom"]=data_dict["z700"]-z700_clim
            data_dict["pv700_anom"]=data_dict["pv700"]-pv700_clim
            data_dict["tcc_anom"]=data_dict["tcc"]-tcc_clim

        
        # ONLY FOR CONTROL
        elif fields=="tracers":
            data_dict["rainEvap"]=load_file("control",date,samp_hr+sim_day*24,"moisture-tracers",stash_dict["traceRainEvap"])[0,2,:2200,:3300] # corresponds to ~50m above surface
            data_dict["boundaryLayer"]=load_file("control",date,samp_hr+sim_day*24,"moisture-tracers",stash_dict["traceBL"])[0,2,:2200,:3300]
            data_dict["boundaryLayerSM"]=load_file("control",date,samp_hr+sim_day*24,"moisture-tracers",stash_dict["traceBLland"])[0,2,:2200,:3300]


        elif fields=="MCSstructure":
            ####
            #field_vars=["t_plevs_anom","z_plevs_anom","u_plevs","v_plevs","q_plevs_anom","omega_plevs","horizontal_divergence_plevs",
            #    "theta_e_plevs_anom","theta_v_plevs","buoyancy_plevs","u_plevs_rel","v_plevs_rel"]
            ####
            # Here using different spatial slices to save memory - they can be closer to edge of Sahel domain since we are taking smaller spatial composites. (+/- 300km)
            u=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["u_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None)) # slice loses 950 hPa
            v=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["v_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
            q=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["q_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
            t=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["t_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
            z=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["z_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
            pv=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["pv_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))
            om=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["omega_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,None))

            
            t=t.where(t!=0)
            q=q.where(q!=0)
            data_dict["t_plevs_anom"]=t - t_clim
            data_dict["z_plevs_anom"]=z.where(z!=0) - z_clim            
            pvals=np.ones(t.shape)
            for i,plev in enumerate(t.pressure.values):
                pvals[i,:,:]=plev*pvals[i,:,:]
                
            data_dict["u_plevs"]=u.where(u!=0)
            data_dict["v_plevs"]=v.where(v!=0)
            data_dict["omega_plevs"]=om.where(t!=0)
            data_dict["q_plevs_anom"]=q - q_clim
            data_dict["pv_plevs_anom"]=pv.where(pv!=0)
            div=mpcalc.divergence(data_dict["u_plevs"],data_dict["v_plevs"],crs=ccrs.PlateCarree())         
            data_dict["horizontal_divergence_plevs"]=div.metpy.dequantify()
            

        elif fields=="instability":
            q=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["q_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,lid+1))
            t=load_file(sim,date,samp_hr+sim_day*24,"model-diagnostics",stash_dict["t_plevs"]).rename({"PLEVS":"pressure"}
                                                ).sel(latitude=slice(6,22),longitude=slice(345,381)).isel(time=0,pressure=slice(1,lid+1))
            pvals=np.ones(t.shape)
            for i,plev in enumerate(t.pressure.values):
                pvals[i,:,:]=plev*pvals[i,:,:]
                
            td=mpcalc.dewpoint_from_specific_humidity(pvals*units("hPa"),t.where(t!=0),q.where(q!=0))
            te=mpcalc.equivalent_potential_temperature(pvals*units("hPa"),t.where(t!=0),td).metpy.dequantify()
            # CAPE proxy is the difference between saturated theta-e and 925hPa theta-e
            td_sat=mpcalc.dewpoint_from_relative_humidity(t,np.ones(q.shape))
            te_sat=mpcalc.equivalent_potential_temperature(pvals*units("hPa"),t.where(t!=0),td_sat).metpy.dequantify()
            data_dict["theta_e_plevs"]=te
            data_dict["theta_e_plevs_anom"]=te - te_clim
            data_dict["capeprox_plevs"]=-1*(te_sat - te.sel(pressure=925))
            data_dict["t_plevs"]=t
        
        
        ################### SAMPLING ROUTINE ###############################
        
        for i, core_loc in enumerate(day_data[[loc_y,loc_x]].values):
            storm=day_data.iloc[i]
            for var in field_vars:
                # extract numpy array of values about each sample point (lat, lon). data_dict entry is full field
                arr=composite_field(data_dict[var], core_loc[0], core_loc[1], size=csize)
                # update same-sized array stored in comp. Starts as zeros. nansum important to account for sub-surface pressure levels etc
                comp[var] = np.nansum((comp[var],arr), axis=0)
                # we ultimately want to compute a mean, so also track sample size. Needs to be grid-level to account for spatial patterns of nans.
                comp[var+"_count"] = comp[var+"_count"] + (~np.isnan(arr)).astype(int)

                # specific computation for rainfall_accums - look for all points where rain accum > 5mm/hr.
                if var+"_prob" in list(comp.keys()):
                    comp[var+"_prob"] = comp[var+"_prob"] + np.where(arr>5,1,0)

                # for most anomalies, the key is in comp only. Thus treat separately; does therefore cause an issue if ever have var and var+"_anom" in field_vars
                if var+"_anom" in list(comp.keys()):
                    arr=composite_field(data_dict[var+"_anom"], core_loc[0], core_loc[1], size=csize)
                    comp[var+"_anom"] = np.nansum((comp[var+"_anom"],arr), axis=0)
                    
        ########################################################################

    
    ##### FINALISE COMPOSITE #####
    # Once all days in period_segment have been sampled, convert arrays in comp to dataArrays
    # Conditionals account for different vertical coordinates (rho and theta being the two model-level sets; such vars specify them in label)
    # Horizontal coordinates applied as kilometres as from centre, from -600 to 600
    for field in list(comp.keys()):
        if "plevs" in field:
            x=int(csize/1.335) # enables flexibility for smaller but taller sections (MCS_sampling) - parameter is grid specific!!
            comp[field]=xr.DataArray(comp[field],coords=[data_dict["t_plevs"].pressure.values,np.arange(-x,x+1,1.5),np.arange(-x,x+1,1.5)],
                                dims=["pressure","latitude","longitude"],name=field)
        elif "rho" in field:
            coord=field.split("_")[-1]
            comp[field]=xr.DataArray(comp[field],coords=[(data_dict["u_rho"].RO_1_40_zsea_rho + data_dict["u_rho"].RO_1_40_C_rho * data_dict["u_rho"].RO_1_40_eta_rho).values,
                                                         np.arange(-600,601,1.5),np.arange(-600,601,1.5)],dims=["height_rho","latitude","longitude"],name=field)
        elif "theta" in field:
            try:
                comp[field]=xr.DataArray(comp[field],coords=[(data_dict["w_theta"].TH_1_80_zsea_theta + data_dict["w_theta"].TH_1_80_C_theta * data_dict["w_theta"].TH_1_80_eta_theta).values,
                                                             np.arange(-600,601,1.5),np.arange(-600,601,1.5)],dims=["height_theta","latitude","longitude"],name=field)
            except:
                comp[field]=xr.DataArray(comp[field],coords=[(data_dict["pt_theta"].TH_0_40_zsea_theta + data_dict["pt_theta"].TH_0_40_C_theta * data_dict["pt_theta"].TH_0_40_eta_theta).values,
                                                             np.arange(-600,601,1.5),np.arange(-600,601,1.5)],dims=["height_theta","latitude","longitude"],name=field)
        else:
            comp[field]=xr.DataArray(comp[field],coords=[np.arange(-600,601,1.5),np.arange(-600,601,1.5)],
                            dims=["latitude","longitude"],name=field)

    # Convert comp dictionary (now one dataArray per variable + its count + its anom) into a single dataset
    comp=[comp[field] for field in list(comp.keys())]
    comp=xr.merge(comp)
    # Output - a dataset of composites for each period_segment
    return comp


################## ACTIVATE PARALLELISE ###########################
# psize is set at argparse input. Default 10
p = Pool(psize)
print(psize)
# Pool returns list of datasets. The indices split full period into 4 day period_segments.
part_composites=p.map(parallelise,np.arange(psize))
out=xr.concat(part_composites,dim="part")
count=0

################## CLEAN UP OUTPUTS ############################

# Core clean-up. Combine the collated sums and counts; combine to take mean; include anomalies and probabilities
if args.split is None:
    for field in field_vars:
        N=out[field+"_count"].sum(dim="part").values
        out[field]=out[field].sum(dim="part")/N
        count=max(count,np.max(N))
        out=out.drop_vars([field+"_count"])
    
        if field+"_anom" in list(out.data_vars):
            out[field+"_anom"]=out[field+"_anom"].sum(dim="part")/N
    
        if field+"_prob" in list(out.data_vars):
            out[field+"_prob"]=out[field+"_prob"].sum(dim="part")/N

    out.attrs["number_cores"]=count

# If split routine used, do everything except take averages. This allows for accurate combination of the splits when all files output
else:
    for field in field_vars:
        print(field)
        out[field+"_count"]=out[field+"_count"].sum(dim="part")
        out[field]=out[field].sum(dim="part")
    
        if field+"_anom" in list(out.data_vars):
            out[field+"_anom"]=out[field+"_anom"].sum(dim="part")
    
        elif field+"_prob" in list(out.data_vars):
            out[field+"_prob"]=out[field+"_prob"].sum(dim="part")



if args.fieldsim is not None:
    fields=fieldsim+fields

if "diffPmax" in method:
    start_str=sim2+"_%02dzSM"%loc_hr
else:
    start_str=sim2+"_%02dzMCS"%loc_hr

# If arg.split has been used, you get a separate .nc file per split
if args.split is None:
    out.to_netcdf("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/composites/Control48_climatology/{}_{}_{}_{}_{:02d}z_composites.nc".format(start_str,fields,filtstr,method,samp_hr))
else:
    out.to_netcdf("/gws/nopw/j04/lmcs/bmaybee/lmcs_run_outputs/composites/Control48_climatology/{}_{}_{}_{}_{:02d}z_composites-P{}.nc".format(start_str,fields,filtstr,method,samp_hr,args.split))
    
